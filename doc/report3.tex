\documentclass{article}
\pagestyle{plain}

\title{Data Prefetching Championship\\
        \large{Lab3-Advanced Memory Systems}}
\author{Ramyad Hadidi}
\date{\today}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}

\begin{document}

\definecolor{light-blue}{RGB}{216,208,255}
\lstset{language=C++, 
	basicstyle=\fontsize{9}{9}\selectfont\ttfamily,
        keywordstyle=\color{Blue}\ttfamily,
        stringstyle=\color{Red}\ttfamily,
        commentstyle=\color{ForestGreen}\ttfamily,
        breaklines=true
	backgroundcolor=\color{light-blue}, 
	frame=single,
	numbers=left, 
	stepnumber=1,
	numberstyle=\tiny\color{Sepia},
	morekeywords={}}

\maketitle

\begin{abstract}
This is the report for Lab3 assignment for Advanced Memory Systems (ECE-7103A) taught by Moinuddin Qureshi on Spring 2015. This assignment is about data prefetching policies. The framework is provided by second data prefetching championship (DPC2). The framework has some traces and basic policies. In this assignment we will measure the performance and sensitivity to prefetching degree of these policies with provided traces. Then, we will develop an online mechanism to change these policies during execution. Finally, we will design a new policy of our own. 
\end{abstract}

\section{Introduction}
In this lab we experiment with different data prefetching policies. Simulation framework is provided by second data prefetching championship (DPC2), available on \url{http://comparch-conf.gatech.edu/dpc2/}. In this section we get familiar with policies, workloads, system configuration and how to execute simulation. In the following sections, first we measure the performance of these four prefetchers and compare them with no prefetching performance. Second, we will change the degree of provided prefetchers and measure the sensitivity of each workload to aggressiveness of the prefetcher.

\subsection{Included Policies}
Provided framework has these three included policies:
\begin{description}
 \item [Next Line Prefetcher]: A simple prefetcher which will prefetch next address of current accessed cache line.
 \item [Program Counter Based Stride Prefetcher]: A stride prefetcher which will detect same strides coming from a single program counter and then prefetches additional lines.
 \item [Stream Prefetcher]: This prefetcher monitors a window of addresses in a page. After confirming the direction of the accesses, it prefetches additional lines.
 \item [Access Map Pattern Matching (AMPM) Prefetcher]: This prefetcher monitors a page for accesses. Then, it tries to match a pattern to the accesses. If the pattern matches, it do the prefetching. Patterns are offset prefetches with different offsets. However, this prefetcher is the lite version of the original AMPM prefetcher which has a larger monitoring address area. 
\end{description}

\subsection{Workloads}
The framework has some short traces. For this assignment these traces will be used. They are 8 traces from SPEC CPU 2006 with almost 3~million instructions each. Here is the list of these workloads:
\begin{table}[!h]
\centering
\begin{tabular}{|| c | c ||}
\hline \hline 
gcc & GemsFDTD \\
lbm & leslie3D \\
libquantum & mcf \\
milc & omnetpp \\
\hline \hline
\end{tabular}
\caption{Workloads}
\label{table_bench}
\end{table}

\subsection{System Configurations}
Framework models a simple out-of-order core with three level of caches. The memory model consists of a 3 level cache hierarchy, with an L1 data cache, an L2 data cache, and an L3 data cache. Instruction caching is not modeled. The L1 data cache is 16KB 8-way set-associative with LRU replacement. The L2 data cache is 128KB 8-way set-associative with LRU replacement. The L3 data cache is 16-way set-associative with LRU replacement. The size of L3 cache and bus bandwidth is a configuration parameter. The L2 data cache is inclusive of the L1, and the L3 data cache is inclusive of both the L1 and L2. Prefetcher works at cache line granularity and works in the physical address space only, and is restricted from crossing 4 KB physical page boundaries. The prefetchers can only see the stream of L1 cache misses / L2 cache reads (including those L2 reads caused by an L1 write miss). In other words, the prefetcher lives at the L2 level of cache. For more detailed description look at \url{http://comparch-conf.gatech.edu/dpc2/simulation_infrastructure.html}.

\subsection{Framework}
Prefetchers code are in \verb+example_prefetchers+ directory. The simulator code is provided as a library in \verb+lib+ directory. To compile an executable of the simulator below command is used:
\footnotesize\begin{verbatim}
 gcc -Wall -o dpc2sim example_prefetchers/stream_prefetcher.c lib/dpc2sim.a
\end{verbatim}\normalsize
Simulator read traces from stdin. So to execute a gzip trace:
\footnotesize\begin{verbatim}
 zcat trace.dpc.gz | ./dpc2sim
\end{verbatim}\normalsize
For easiness, I have written a python script, \verb+run.py+ to compile and execute different prefetches in \verb+example_prefetchers+ directory. Below is the help of this script:
\footnotesize\begin{verbatim}
 usage: run.py [-h] [--dryRun] [-o OUTPUTDIR] [-e [EXE [EXE ...]]] [-s]
              [-d DEGREE]

run.py

optional arguments:
  -h, --help            show this help message and exit
  --dryRun              Print out the generated script(s) instead of launching
                        jobs
  -o OUTPUTDIR, --outputDir OUTPUTDIR
                        Root directory for result files
  -e [EXE [EXE ...]], --exe [EXE [EXE ...]]
                        Executables to run
  -s, --submit          Submit Jobs to qsub
  -d DEGREE, --degree DEGREE
                        Degree of Prefetcher, for name generation
\end{verbatim}\normalsize
This script save the output of the simulator to a directory. After that, finding the corresponding performance is easy with \verb+grep+ command.


\section{Policies Performance}
Figure \ref{pic:perf} shows the performance speedups for different policies. X axis is the traces and Y axis is speedup based on IPC. Positive values shows speed up and negative values shows slow downs. AMPM is the best policy across all other policies. The online policy is my own designed mechanism to detect best policy in execution. 

\begin{figure}[h!]
  \label{pic:perf}
  \centering
    \includegraphics[width=1\textwidth]{perf.png}
    \caption{Policies Speedup in IPC - Normalized to No Prefetching}
\end{figure}

\section{Sensitivity of Workloads to Prefetcher Aggressiveness}
In this section we evaluate the sensitivity of each workload to the aggressiveness of prefetchers. Aggressiveness is defined as prefetching degree in the code. It means that how many prefetches will be done for a demand access. In following subsections we investigate this for different policies.

\subsection{Next Line}
As figure \ref{pic:next-agg} shows, increasing next line prefetcher degree causes cache pollution and also a significant negative speedup. This happens mostly for omnetpp, mcf and gcc which as next subsection will show have not a very good detectable accesses. Therefore, when increasing the degree it will cause the useful cache lines to be evicted. lbm and libquantum are only workloads that get speedup with higher degree. As figure \ref{pic:stream-agg} shows, these two has a good speedup in stream prefetcher too. This suggest that these workload accesses have a locality which next line prefetcher exploits. However, we can not see the same result for leslie3D. This could be because of negative distance accesses in leslie3D. Therefore, leslie3D can get speedup in stream prefetcher and not next line prefetcher (which only prefetches next lines and not previous lines). GemsFDTD probably has a random access pattern, since non of the prefetchers have speedup with GemsFDTD (figure \ref{pic:perf}). For milc workload, it is possible have different patterns as figure \ref{pic:perf} shows for prefetcher stream and ip-stride. However, increasing next-line prefetcher's degree caused more pollution than gains.

\begin{figure}[!h]
  \label{pic:next-agg}
  \centering
    \includegraphics[width=1\textwidth]{next_agg.png}
    \caption{Next Line Policy Aggressiveness impact on Speedup - Normalized to degree one}
\end{figure}

\subsection{IP Stride}
The results is in figure \ref{pic:ip-agg}. As we can see except milc workload, almost not other workloads get the speedup with higher degree for ip-stride prefetcher. Probably this is because almost all access are stride accesses in milc. This can be seen in figure \ref{pic:perf} as the speed up of a more complex pattern detector like AMPM is the same as simple ones like ip-stride prefetcher. Degradation of speedup in other workloads it is probably because ip-stride prefetcher has no confirmation based prefetches like stream prefetcher (\ref{pic:stream-agg}). As for stream prefetcher could be seen, because this prefetcher need confirmation its degradation with degree is less than ip-stride. 

\begin{figure}[!h]
  \label{pic:ip-agg}
  \centering
    \includegraphics[width=1\textwidth]{ip_agg.png}
    \caption{IP Stride Policy Aggressiveness impact on Speedup - Normalized to degree one}
\end{figure}

\subsection{Stream}
For stream prefetcher, workloads lbm, leslie3D and milc get the most benefit with prefetcher degree increments. milc, as we talked about before, is consist of strided accesses therefore it gets benefit with more degree. lbm is the only benchmark that gets more benefit for next-line prefetcher (figure \ref{pic:next-agg}), it suggest that it will also get benefits from stream prefetcher. Since, stream prefetcher is like a confirmation based next/previous line prefetcher. gcc, omnetpp, GemsFDTD and mcf are all like each other. Stream prefetcher cannot detect their misses. Also, gcc has a negative speedup in stream prefetcher. This is probably because gcc have a random locality, therefore cache pollution (which is stream is higher) has more impact.

\begin{figure}[!h]
  \label{pic:stream-agg}
  \centering
    \includegraphics[width=1\textwidth]{stream_agg.png}
    \caption{Stream Policy Aggressiveness impact on Speedup - Normalized to degree one}
\end{figure}

\subsection{AMPM}
For AMPM larger degrees act as degree of two as seen in figure \ref{pic:ampm-agg}. This could be because of various reasons. First, it would be because there is not matching in the patters. Or, there as a match, since the code has a lot of breaks in it, it usually does not prefetch the data. This is because this version of AMPM monitors a smaller indexes and also it can only fetch in the current working memory page.

\begin{figure}[!h]
  \label{pic:ampm-agg}
  \centering
    \includegraphics[width=1\textwidth]{ampm_agg.png}
    \caption{AMPM Policy Aggressiveness impact on Speedup - Normalized to degree one}
\end{figure}

\section{Online Prefetching Mechanism}
I tired different kinds of ideas to change prefetcher during execution. I included two of them here: Parallel sandboxes and page dueling. I used parallel sandboxes for my online policy. However, to be able to get the performance near AMPM I had to add many others features. This problem of lower performance in online policy raise mainly because:
\begin{itemize}
 \item AMPM is a mix of Stream and Next-Line prefetcher, therefore it is not a good idea to mix these three together.
 \item I understand that Next-Line prefetches usually will be hit in the cache. This is because high locality of access.
 \item Also, amount of traffic generated by IP stride and Next-Line prefetcher is usually high, therefore their relative number of hits are higher 
\end{itemize}
In following subsections I have described my online policies and also how I overcame these problems in my implementations. 

\subsection{Parallel Sandboxes}
I get the idea for this mechanism from the last paper about prefetchers in HPCA 2014, ''Sandbox Prefetching: Safe Run-Time Evaluation of Aggressive Prefetchers``. However, since there is no budget limitations, I implemented the parallel version of this paper to evaluate four different prefetchers. General mechanism is done on every fixed number of L2 accesses. In every period, each prefetchers imports its prefetches lines in their own sandbox. In subsequent accesses, we also check these sandboxes to see if a prefetcher predict a line correctly or not. If yes, we will increase the score of that prefetcher. In the end of that period, we make decision about the next period's prefetcher based on these scores. To overcame the problems I mentioned above I change the sandboxes like this:
\begin{itemize}
 \item I always carry a fraction of score from previous period to the current period. Current coefficient is 3, which means 1/3 of the last period's score will make the new score. This will help to detect the best prefetcher in a longer monitoring period ans omit large spikes.
 \item I edited next-line prefetcher to only operate on cache-misses since this prefetcher's performance is not very good in compare to AMPM. This is because of high locality, if we hit the current line, it is possible to hit the next-line to. With this change I reduced the number of prefetch address generated by this prefetches, therefore this prefetcher will not get a higher score because of higher locality. 
 \item To omit high number of locality hits, I have delayed score counting a period later. Therefore, I will actually count the scores for prefetches that are really separated in time. Since the closer the access are, it is more probable that we have the cache line already in our cache.
\end{itemize}

\subsection{Page Dueling}
In this model we will monitor different pages which will use a dedicated prefetcher, and based on the performance they get for that page we will extend the decision to the other pages. However, there are many complications. First, programs usually use some pages and accesses are not spread out between all pages. Second, it is highly probable that a program only operate in a single page for a short period, therefore it is really hard to make a decision. To solve this problem, we could divide each page between our prefetchers. However, since our prefetchers usually work in a page limit, this separation will have a negative impact on their performance.


\section{Proposed Improved Prefetcher}

% \clearpage
% 
% \bibliographystyle{plain}
% \bibliography{refrences}
\end{document}
